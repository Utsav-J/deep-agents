"""Prompt templates and tool descriptions for deep agents from scratch.

This module contains all the system prompts, tool descriptions, and instruction
templates used throughout the deep agents educational framework.
"""

WRITE_TODOS_DESCRIPTION = """Create and manage structured task lists for tracking progress through complex workflows.

## When to Use
- Multi-step or non-trivial tasks requiring coordination
- When user provides multiple tasks or explicitly requests todo list  
- Avoid for single, trivial actions unless directed otherwise

## Structure
- Maintain one list containing multiple todo objects (content, status, id)
- Use clear, actionable content descriptions
- Status must be: pending, in_progress, or completed

## Best Practices  
- Only one in_progress task at a time
- Mark completed immediately when task is fully done
- Always send the full updated list when making changes
- Prune irrelevant items to keep list focused

## Progress Updates
- Call TodoWrite again to change task status or edit content
- Reflect real-time progress; don't batch completions  
- If blocked, keep in_progress and add new task describing blocker

## Parameters
- todos: List of TODO items with content and status fields

## Returns
Updates agent state with new todo list."""

TODO_USAGE_INSTRUCTIONS = """Based upon the user's request:
1. Use the write_todos tool to create TODO at the start of a user request, per the tool description.
2. After you accomplish a TODO, use the read_todos to read the TODOs in order to remind yourself of the plan. 
3. Reflect on what you've done and the TODO.
4. Mark you task as completed, and proceed to the next TODO.
5. Continue this process until you have completed all TODOs.

IMPORTANT: Always create a research plan of TODOs and conduct research following the above guidelines for ANY user request.
IMPORTANT: Aim to batch research tasks into a *single TODO* in order to minimize the number of TODOs you have to keep track of.
"""

LS_DESCRIPTION = """List all files in the virtual filesystem stored in agent state.

Shows what files currently exist in agent memory. Use this to orient yourself before other file operations and maintain awareness of your file organization.

No parameters required - simply call ls() to see all available files."""

READ_FILE_DESCRIPTION = """Read content from a file in the virtual filesystem with optional pagination.

This tool returns file content with line numbers (like `cat -n`) and supports reading large files in chunks to avoid context overflow.

Parameters:
- file_path (required): Path to the file you want to read
- offset (optional, default=0): Line number to start reading from  
- limit (optional, default=2000): Maximum number of lines to read

Essential before making any edits to understand existing content. Always read a file before editing it."""

WRITE_FILE_DESCRIPTION = """Create a new file or completely overwrite an existing file in the virtual filesystem.

This tool creates new files or replaces entire file contents. Use for initial file creation or complete rewrites. Files are stored persistently in agent state.

Parameters:
- file_path (required): Path where the file should be created/overwritten
- content (required): The complete content to write to the file

Important: This replaces the entire file content."""

FILE_USAGE_INSTRUCTIONS = """You have access to a virtual file system to help you retain and save context.

## Workflow Process
1. **Orient**: Use ls() to see existing files before starting work
2. **Save**: Use write_file() to store the user's request so that we can keep it for later 
3. **Research**: Proceed with research. The search tool will write files.  
4. **Read**: Once you are satisfied with the collected sources, read the files and use them to answer the user's question directly.
"""

SUMMARIZE_WEB_SEARCH = """You are creating a minimal summary for research steering - your goal is to help an agent know what information it has collected, NOT to preserve all details.

<webpage_content>
{webpage_content}
</webpage_content>

Create a VERY CONCISE summary focusing on:
1. Main topic/subject in 1-2 sentences
2. Key information type (facts, tutorial, news, analysis, etc.)  
3. Most significant 1-2 findings or points

Keep the summary under 150 words total. The agent needs to know what's in this file to decide if it should search for more information or use this source.

Generate a descriptive filename that indicates the content type and topic (e.g., "mcp_protocol_overview.md", "ai_safety_research_2024.md").

Output format:
```json
{{
   "filename": "descriptive_filename.md",
   "summary": "Very brief summary under 150 words focusing on main topic and key findings"
}}
```

Today's date: {date}
"""

RESEARCHER_INSTRUCTIONS = """You are a research assistant conducting research on the user's input topic. For context, today's date is {date}.

<Task>
Your job is to use tools to gather information about the user's input topic.
You can use any of the tools provided to you to find resources that can help answer the research question. You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.
</Task>

<Available Tools>
You have access to two main tools:
1. **tavily_search**: For conducting web searches to gather information
2. **think_tool**: For reflection and strategic planning during research

**CRITICAL: Use think_tool after each search to reflect on results and plan next steps**
</Available Tools>

<Instructions>
Think like a human researcher with limited time. Follow these steps:

1. **Read the question carefully** - What specific information does the user need?
2. **Start with broader searches** - Use broad, comprehensive queries first
3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?
4. **Execute narrower searches as you gather information** - Fill in the gaps
5. **Stop when you can answer confidently** - Don't keep searching for perfection
</Instructions>

<Hard Limits>
**Tool Call Budgets** (Prevent excessive searching):
- **Simple queries**: Use 1-2 search tool calls maximum
- **Normal queries**: Use 2-3 search tool calls maximum
- **Very Complex queries**: Use up to 5 search tool calls maximum
- **Always stop**: After 5 search tool calls if you cannot find the right sources

**Stop Immediately When**:
- You can answer the user's question comprehensively
- You have 3+ relevant examples/sources for the question
- Your last 2 searches returned similar information
</Hard Limits>

<Show Your Thinking>
After each search tool call, use think_tool to analyze the results:
- What key information did I find?
- What's missing?
- Do I have enough to answer the question comprehensively?
- Should I search more or provide my answer?
</Show Your Thinking>
"""

TASK_DESCRIPTION_PREFIX = """Delegate a task to a specialized sub-agent with isolated context. Available agents for delegation are:
{other_agents}
"""

SUBAGENT_USAGE_INSTRUCTIONS = """You can delegate tasks to sub-agents.

<Task>
Your role is to coordinate research by delegating specific research tasks to sub-agents.
</Task>

<Available Tools>
1. **task(description, subagent_type)**: Delegate research tasks to specialized sub-agents
   - description: Clear, specific research question or task
   - subagent_type: Type of agent to use (e.g., "research-agent")
2. **think_tool(reflection)**: Reflect on the results of each delegated task and plan next steps.
   - reflection: Your detailed reflection on the results of the task and next steps.

**PARALLEL RESEARCH**: When you identify multiple independent research directions, make multiple **task** tool calls in a single response to enable parallel execution. Use at most {max_concurrent_research_units} parallel agents per iteration.
</Available Tools>

<Hard Limits>
**Task Delegation Budgets** (Prevent excessive delegation):
- **Bias towards focused research** - Use single agent for simple questions, multiple only when clearly beneficial or when you have multiple independent research directions based on the user's request.
- **Stop when adequate** - Don't over-research; stop when you have sufficient information
- **Limit iterations** - Stop after {max_researcher_iterations} task delegations if you haven't found adequate sources
</Hard Limits>

<Scaling Rules>
**Simple fact-finding, lists, and rankings** can use a single sub-agent:
- *Example*: "List the top 10 coffee shops in San Francisco" ‚Üí Use 1 sub-agent, store in `findings_coffee_shops.md`

**Comparisons** can use a sub-agent for each element of the comparison:
- *Example*: "Compare OpenAI vs. Anthropic vs. DeepMind approaches to AI safety" ‚Üí Use 3 sub-agents
- Store findings in separate files: `findings_openai_safety.md`, `findings_anthropic_safety.md`, `findings_deepmind_safety.md`

**Multi-faceted research** can use parallel agents for different aspects:
- *Example*: "Research renewable energy: costs, environmental impact, and adoption rates" ‚Üí Use 3 sub-agents
- Organize findings by aspect in separate files

**Important Reminders:**
- Each **task** call creates a dedicated research agent with isolated context
- Sub-agents can't see each other's work - provide complete standalone instructions
- Use clear, specific language - avoid acronyms or abbreviations in task descriptions
</Scaling Rules>"""


EMULATED_WEB_SEARCH_RESULTS = """
Sure ‚Äî here‚Äôs a clear and complete overview of **MCP (Model Context Protocol)**:

---

### üß© What is MCP?

**MCP (Model Context Protocol)** is an **open protocol developed by Anthropic** that defines how **AI models (like Claude or GPT-based agents)** can interact with **external data sources, tools, or services** in a standardized way.

It provides a **structured, model-agnostic interface** for models to **retrieve, send, and update context** ‚Äî such as documents, APIs, databases, or even other AI agents.

In simple terms:

> MCP lets your AI agent ‚Äúplug in‚Äù to any external system (data, API, or tool) through a standard protocol ‚Äî instead of writing ad-hoc integrations for each one.

---

### ‚öôÔ∏è Core Idea

Traditionally, models are limited to the text you send them (prompt context). MCP extends this by allowing models to **dynamically access external context** through well-defined endpoints.

It introduces:

* **Servers** ‚Üí Expose tools or data.
* **Clients** ‚Üí (like your AI agent) connect to those servers.
* **Transport layer** ‚Üí (e.g., HTTP or WebSocket) that handles communication.

---

### üèóÔ∏è MCP Architecture Overview

| Component      | Role                                                                                         | Example                                                        |
| -------------- | -------------------------------------------------------------------------------------------- | -------------------------------------------------------------- |
| **MCP Server** | Exposes a set of *tools* or *resources* (APIs, DBs, documents) in a standard format.         | e.g., a document search service, or a database of transactions |
| **MCP Client** | The AI agent or runtime that connects to one or more servers and uses their tools/resources. | e.g., a LangChain or Claude agent                              |
| **Transport**  | Communication channel between client and server.                                             | Streamable HTTP, WebSocket, or ASGI                            |
| **Tool**       | Action endpoint a model can invoke (like a function call).                                   | `get_exchange_rate()`, `query_vector_store()`                  |
| **Resource**   | Data source the model can read (static or live context).                                     | `/documents`, `/transactions`                                  |

---

### üîå Example Flow

1. Your **AI agent** connects to two MCP servers:

   * One serving **vector search** for documents.
   * One serving **transactional data** (like bank trades).

2. When a user asks:

   > ‚ÄúWhat was our USD trading volume last quarter compared to last year?‚Äù

3. The agent:

   * Uses MCP to **query the transaction server** for structured data.
   * Uses the **document server** for context on quarterly performance.
   * Combines both and generates a natural-language answer.

This way, the agent dynamically accesses real data ‚Äî not just what‚Äôs in its prompt.

---

### üß† Why MCP Matters

| Advantage                  | Description                                                                                              |
| -------------------------- | -------------------------------------------------------------------------------------------------------- |
| **Decoupled Architecture** | No more hardcoded integrations ‚Äî tools can be added/removed without retraining or modifying the model.   |
| **Interoperability**       | Any AI system (Claude, GPT, etc.) can talk to any MCP server ‚Äî universal compatibility.                  |
| **Security and Control**   | You can sandbox servers and control what data models can access.                                         |
| **Composability**          | Multiple servers can expose specialized capabilities (RAG, analytics, transactions, etc.) for one agent. |

---

### üíª Example (Python MCP SDK)

```python
from mcp.server.fastapi import MCPServer

server = MCPServer("transaction_server")

@server.tool("get_usd_volume")
async def get_usd_volume(start_date: str, end_date: str):
    # Access your DB or API
    return {"volume": 1530000, "currency": "USD"}
```

Then, your client agent connects to this MCP server:

```python
from mcp.client import MCPClient

client = MCPClient("http://localhost:8081/mcp")
tool = client.get_tool("get_usd_volume")
result = await tool.invoke({"start_date": "2025-01-01", "end_date": "2025-03-31"})
```

---

### üß© Integration Example (with LangChain)

You can use:

```python
from langchain_mcp_adapters import load_mcp_tools, create_react_agent
```

This allows a LangChain agent to automatically discover and use all tools exposed by MCP servers.

---

### üåç Transport Options

* **HTTP Streamable (default)** ‚Äî lightweight and compatible with web clients.
* **ASGI** ‚Äî integrates directly with FastAPI or other async Python servers.
* **WebSocket** ‚Äî for live or bidirectional communication.

---

### üöÄ Common Use Cases

* **Agentic RAG** ‚Äî connect a model to multiple data backends.
* **Enterprise AI connectors** ‚Äî expose internal APIs securely.
* **Multi-agent collaboration** ‚Äî allow multiple AI agents to communicate through MCP.
* **Model evaluation pipelines** ‚Äî connect generation agents with critique/validation servers.

---

Would you like me to include a **diagram** showing how MCP clients, servers, and agents interact (like in an agentic RAG setup)?

"""